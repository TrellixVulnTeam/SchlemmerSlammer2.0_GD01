{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SchlemmerSlammer - Neural Network for recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Teammembers\n",
    "#### Maxim Bex\n",
    "research, documentation, concept, coding\n",
    "#### Hannes Gelbhardt\n",
    "coding, research, concept, documentation\n",
    "#### York Smeddinck\n",
    "documentation, concept, coding, research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Problem Description\n",
    "We thought about training an AI to recommend / create recipes based on given attributes (e.g. rating, certain ingredients (include / exclude), calories, sodium, fat, FODMAP, etc.pp.). The following graphic is an entity relationship diagram for recipes, which we created to get a better understanding of the nature and complexity of our problem.\n",
    "\n",
    "Since this would be a rather complex system to begin with, we decided to break the problem down, and start with a smaller application first. Thus we are trying to train a neural network to recommend recipe ingredients based on given ingredients.\n",
    "\n",
    "\n",
    "#### General Knowledge\n",
    "One of the more basic problems we face when we talking about natural language processing is that - in difference to humans - computers can just understand numbers, more specific ones and zeroes. It is not too hard to represent a language's syntax in numbers, but if we try to get meaning and context into language, humans are way better in understanding each other. The main reason for that is that humans often times have a similar context (background knowledge) so sometimes they even can communicate without words. To make this more clear, when we talk about cooking each human probably have a similar concept of preparing food in mind, but a computer just understands the word cooking out of it’s language context without actual knowing or understanding what it means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset description\n",
    "### Source: https://www.kaggle.com/hugodarwood/epirecipes\n",
    "The Dataset is a collection of about 21000 recipes given in a .csv and .json format, where the json file contained all information of the recipes and the .csv file is basically a list of attributes and ingredients for all recipes. We opted for using the json file, which includes more context of the ingredients (e.g. 3 evenly chopped tomatoes) compared to the csv file (e.g. 3 tomatoes), which should result in a more complex model.\n",
    "### Processing the data:\n",
    "Because we put our focus on the relationship of ingredients, we preprocess the data from the json format into a textfile consisting of single line listings of the recipes ingredients, excluding special characters (see variable badchars in the code below.)\n",
    "#### Problems during preprocessing\n",
    "The first problem was that the windows command line could not process specific characters of the dataset, which first was solved by setting the command line charset to \"UTF - 8\" Unfortunately we just could read and process about 700 recipes, but not all.\n",
    "\n",
    "Our solution to this was to do all the preprocessing through a python script (see below).\n",
    "\n",
    "Another problem were the unwanted “bad” characters (basically all symbols that are not letters) and how to exclude them.\n",
    "Our solution is to check every single word for said bad characters and to replace those with an empty space.\n",
    "\n",
    "Also we discussed whether to keep numbers (e.g. ¾) and measurement units (e.g. tbsp or cup), but we decided that our neural network would probably benefit from being able to put those aspects into context as well,so we kept them included. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing for our interpreter of the neural network\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "badchars = set(\"0123456789(),-*\\\"'<>|:/%;\\\\\")\n",
    "\n",
    "with open('full_format_recipes.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    outFile = \"./preProcessedData.txt\"\n",
    "    with codecs.open (outFile, \"w\", \"utf-8-sig\") as f:\n",
    "        for recipe in data:\n",
    "            try:\n",
    "                ingredients = recipe['ingredients']\n",
    "                for ingredient in ingredients:\n",
    "                    for c in badchars:\n",
    "                        ingredient = ingredient.replace(c,'')\n",
    "                    if len(ingredient) > 1:\n",
    "                        try:\n",
    "                            f.write(ingredient+\" \")\n",
    "                        except UnicodeEncodeError:\n",
    "                            print(\"UnicodeEncodeError\")\n",
    "                f.write(\"\\n\")\n",
    "            except KeyError:\n",
    "                print(\"KeyError: No ingredients listed in the recipe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import codecs\n",
    "import d2l\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "import math\n",
    "from mxnet import autograd, gluon, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
    "import random\n",
    "import sys\n",
    "import collections\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#For the visualisation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#For Glove and Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the number of lines to get an idea how many recipes we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cups',\n",
       "  'lowsodium',\n",
       "  'vegetable',\n",
       "  'or',\n",
       "  'chicken',\n",
       "  'stock',\n",
       "  'cup',\n",
       "  'dried',\n",
       "  'brown',\n",
       "  'lentils',\n",
       "  'cup',\n",
       "  'dried',\n",
       "  'French',\n",
       "  'green',\n",
       "  'lentils',\n",
       "  'stalks',\n",
       "  'celery',\n",
       "  'chopped',\n",
       "  'large',\n",
       "  'carrot',\n",
       "  'peeled',\n",
       "  'and',\n",
       "  'chopped',\n",
       "  'sprig',\n",
       "  'fresh',\n",
       "  'thyme',\n",
       "  'teaspoon',\n",
       "  'kosher',\n",
       "  'salt',\n",
       "  'medium',\n",
       "  'tomato',\n",
       "  'cored',\n",
       "  'seeded',\n",
       "  'and',\n",
       "  'diced',\n",
       "  'small',\n",
       "  'Fuji',\n",
       "  'apple',\n",
       "  'cored',\n",
       "  'and',\n",
       "  'diced',\n",
       "  'tablespoon',\n",
       "  'freshly',\n",
       "  'squeezed',\n",
       "  'lemon',\n",
       "  'juice',\n",
       "  'teaspoons',\n",
       "  'extravirgin',\n",
       "  'olive',\n",
       "  'oil',\n",
       "  'Freshly',\n",
       "  'ground',\n",
       "  'black',\n",
       "  'pepper',\n",
       "  'to',\n",
       "  'taste',\n",
       "  'sheets',\n",
       "  'wholewheat',\n",
       "  'lavash',\n",
       "  'cut',\n",
       "  'in',\n",
       "  'half',\n",
       "  'crosswise',\n",
       "  'or',\n",
       "  'inch',\n",
       "  'flour',\n",
       "  'tortillas',\n",
       "  'pound',\n",
       "  'turkey',\n",
       "  'breast',\n",
       "  'thinly',\n",
       "  'sliced',\n",
       "  'head',\n",
       "  'Bibb',\n",
       "  'lettuce'],\n",
       " ['cups',\n",
       "  'whipping',\n",
       "  'cream',\n",
       "  'medium',\n",
       "  'onions',\n",
       "  'chopped',\n",
       "  'teaspoons',\n",
       "  'salt',\n",
       "  'bay',\n",
       "  'leaves',\n",
       "  'whole',\n",
       "  'cloves',\n",
       "  'large',\n",
       "  'garlic',\n",
       "  'clove',\n",
       "  'crushed',\n",
       "  'teaspoon',\n",
       "  'pepper',\n",
       "  'teaspoon',\n",
       "  'ground',\n",
       "  'nutmeg',\n",
       "  'Pinch',\n",
       "  'of',\n",
       "  'dried',\n",
       "  'thyme',\n",
       "  'crumbled',\n",
       "  'large',\n",
       "  'shallots',\n",
       "  'minced',\n",
       "  'tablespoon',\n",
       "  'butter',\n",
       "  'pound',\n",
       "  'trimmed',\n",
       "  'boneless',\n",
       "  'center',\n",
       "  'pork',\n",
       "  'loin',\n",
       "  'sinew',\n",
       "  'removed',\n",
       "  'cut',\n",
       "  'into',\n",
       "  'inch',\n",
       "  'chunks',\n",
       "  'well',\n",
       "  'chilled',\n",
       "  'eggs',\n",
       "  'tablespoon',\n",
       "  'all',\n",
       "  'purpose',\n",
       "  'flour',\n",
       "  'cup',\n",
       "  'tawny',\n",
       "  'Port',\n",
       "  'tablespoons',\n",
       "  'dried',\n",
       "  'currants',\n",
       "  'minced',\n",
       "  'Lettuce',\n",
       "  'leaves',\n",
       "  'Cracked',\n",
       "  'peppercorns',\n",
       "  'Minced',\n",
       "  'fresh',\n",
       "  'parsley',\n",
       "  'Bay',\n",
       "  'leaves',\n",
       "  'French',\n",
       "  'bread',\n",
       "  'baguette',\n",
       "  'slices',\n",
       "  'tablespoons',\n",
       "  'olive',\n",
       "  'oil',\n",
       "  'large',\n",
       "  'red',\n",
       "  'onions',\n",
       "  'halved',\n",
       "  'sliced',\n",
       "  'tablespoons',\n",
       "  'dried',\n",
       "  'currants',\n",
       "  'tablespoons',\n",
       "  'red',\n",
       "  'wine',\n",
       "  'vinegar',\n",
       "  'tablespoons',\n",
       "  'canned',\n",
       "  'chicken',\n",
       "  'broth',\n",
       "  'teaspoons',\n",
       "  'chopped',\n",
       "  'fresh',\n",
       "  'thyme',\n",
       "  'or',\n",
       "  'teaspoon',\n",
       "  'dried',\n",
       "  'crumbled',\n",
       "  'teaspoon',\n",
       "  'sugar']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading preprocessed data\n",
    "with codecs.open('./preProcessedData.txt', 'r', 'utf-8-sig') as f:\n",
    "    lines = f.readlines()\n",
    "    ingredients = [st.split() for st in lines]\n",
    "'# sentences: %d' % len(lines)\n",
    "ingredients[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearranging all ingredients into a single array, for later comparison\n",
    "ingredients_single = []\n",
    "for recipe in ingredients:\n",
    "    for item in recipe:\n",
    "        if item not in ingredients_single:\n",
    "            ingredients_single.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more meaningful result, delete the tokens, which come less then 3 times in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rare tokens\n",
    "counter = collections.Counter([tk for st in ingredients for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 3, counter.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count the total number of tokens in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 915637'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokencount\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in ingredients]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "'# tokens: %d' % num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the relationship between the most frequent words and the less frequent ones more meaningful and have a better performance, we used subsampling and print out the result of the reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 256288'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subsampling\n",
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison we count the occurrence of the frequently used word \"cup\" before and after subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# cup: before=43072, after=1893'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare token frequency before and after subsampling for the word \"cup\"\n",
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (token, sum(\n",
    "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
    "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
    "\n",
    "compare_counts('cup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining contexts of each word in the dataset\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "#demonstration of context\n",
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps we implement the Skip-Gram Model. The first thing to do is trying to give the single words a context based meaning, so that in our previously giving example “I cook carrots” or “I cook potatoes” carrots, potatoes and cooking are closely related. On the top trough subsampling the performance of the later training part will be better, because the dataset is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test example, extracting words to the context max window size of 5\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sampling is a second mechanism to improve the perfomance of the training part and also to get slightly better results. In the word representation after each  “going trough” the weights of all other words except the target word should be updated, which would need an enormous amount of computing power. In negative sampling we just pick a few, randomly chosen words which don’t occur in the context and change there weights related to the target word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative sampling for training\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # Noise words cannot be context words\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using mini batches gradient descent algorithm for the data reading process in a function \n",
    "def batchify(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (nd.array(centers).reshape((-1, 1)), nd.array(contexts_negatives),\n",
    "            nd.array(masks), nd.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: (512, 1)\n",
      "contexts_negatives shape: (512, 60)\n",
      "masks shape: (512, 60)\n",
      "labels shape: (512, 60)\n"
     ]
    }
   ],
   "source": [
    "#We use the previously defined batchify function to specify the data loader instance \n",
    "#and print the shape of each variable into the first batch read\n",
    "batch_size = 512\n",
    "#Checks how many cpu are available\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "dataset = gdata.ArrayDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                             batchify_fn=batchify, num_workers=num_workers)\n",
    "\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter embedding0_weight (shape=(20, 4), dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skip-Gram Model\n",
    "#Embedding a layer with a input size of 20 neurons and a output of 4\n",
    "embed = nn.Embedding(input_dim=20, output_dim=4)\n",
    "embed.initialize()\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[ 0.01438687  0.05011239  0.00628365  0.04861524]\n",
       "  [-0.01068833  0.01729892  0.02042518 -0.01618656]\n",
       "  [-0.00873779 -0.02834515  0.05484822 -0.06206018]]\n",
       "\n",
       " [[ 0.06491279 -0.03182812 -0.01631819 -0.00312688]\n",
       "  [ 0.0408415   0.04370362  0.00404529 -0.0028032 ]\n",
       "  [ 0.00952624 -0.01501013  0.05958354  0.04705103]]]\n",
       "<NDArray 2x3x4 @cpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The input of the embedding layer is the index of the context word\n",
    "x = nd.array([[1, 2, 3], [4, 5, 6]])\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mini Batch Multiplication\n",
    "X = nd.ones((2, 1, 4))\n",
    "Y = nd.ones((2, 4, 6))\n",
    "nd.batch_dot(X, Y).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-Gram Forward Calculation\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = nd.batch_dot(v, u.swapaxes(1, 2))\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use Gluon's binary cross entropy loss function Sigmoid Binary Cross Entropy Loss\n",
    "loss = gloss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.8739896 1.2099689]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mask functions are considerable\n",
    "pred = nd.array([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "# 1 and 0 in the label variables label represent context words and the noise\n",
    "# words, respectively\n",
    "label = nd.array([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
    "mask = nd.array([[1, 1, 1, 1], [1, 1, 1, 0]])  # Mask variable\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8739896\n",
      "1.2099689\n"
     ]
    }
   ],
   "source": [
    "#binary cross-entropy loss function calculation to compare\n",
    "#and calculate the predicted value with a mask of 1 and the loss of the label based on the mask variable mask.\n",
    "def sigmd(x):\n",
    "    return -math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "print('%.7f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4))\n",
    "print('%.7f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Model Parameter with a size of 100, a sequential neural network\n",
    "embed_size = 100\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size),\n",
    "        nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for the training process\n",
    "def train(net, lr, num_epochs):\n",
    "    ctx = d2l.try_gpu()\n",
    "    net.initialize(ctx=ctx, force_reinit=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [\n",
    "                data.as_in_context(ctx) for data in batch]\n",
    "            with autograd.record():\n",
    "                pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "                # Use the mask variable to avoid the effect of padding on loss\n",
    "                # function calculations\n",
    "                l = (loss(pred.reshape(label.shape), label, mask) *\n",
    "                     mask.shape[1] / mask.sum(axis=1))\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            l_sum += l.sum().asscalar()\n",
    "            n += l.size\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN!\n",
    "train(net, 0.005, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives similar token (our neural network)\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data()\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    # The added 1e-9 is for numerical stability\n",
    "    cos = nd.dot(W, x) / (nd.sum(W * W, axis=1) * nd.sum(x * x) + 1e-9).sqrt()\n",
    "    topk = nd.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print('cosine sim=%.3f: %s' % (cos[i].asscalar(), (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('tomato', 10, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Gazt\\\\AppData\\\\Local\\\\Temp\\\\preProcessedData.txt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initilizing glove\n",
    "glove_file = datapath('./glove.6B.100d.txt')\n",
    "glove_file = './glove.6B.100d.txt'\n",
    "\n",
    "word2vec_glove_file = get_tmpfile(\"preProcessedData.txt\")\n",
    "word2vec_glove_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#glove2word2vec\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the glove model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, the trained model gives us pretty good results for word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sauce', 0.7973388433456421),\n",
       " ('avocado', 0.7904785871505737),\n",
       " ('eggplant', 0.7775974869728088),\n",
       " ('cucumber', 0.7695537805557251),\n",
       " ('onion', 0.7660188674926758),\n",
       " ('lettuce', 0.7573791742324829),\n",
       " ('watermelon', 0.7465941309928894),\n",
       " ('potato', 0.7453776597976685),\n",
       " ('soup', 0.735533595085144),\n",
       " ('mango', 0.7307522296905518)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gives similar tokens (glove)\n",
    "model.most_similar('tomato')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next we are creating our recipe function, which generates as many similar ingredients as wanted, based on a set of given ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create recipe function\n",
    "def create_recipe(model,words,sample=0):\n",
    "    return words, model.most_similar(words)[:sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes we define different sets of ingredients..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining 4 sets of words for testing purposes\n",
    "words = ['carrot', 'salt', 'oil', 'chicken','bread']\n",
    "words2 = ['flour', 'egg', 'strawberry']\n",
    "words3 = ['salt', 'rice', 'garlic','curry']\n",
    "words4 = ['banana','curry','rice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and our proposel function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['carrot', 'salt', 'oil', 'chicken', 'bread'],\n",
       " [('butter', 0.8067049980163574),\n",
       "  ('garlic', 0.7763496041297913),\n",
       "  ('onion', 0.7645821571350098),\n",
       "  ('vegetable', 0.7586125731468201),\n",
       "  ('sugar', 0.7478333711624146)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recipe 1\n",
    "create_recipe(model,words,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['flour', 'egg', 'strawberry'],\n",
       " [('cream', 0.795329213142395),\n",
       "  ('butter', 0.7796475887298584),\n",
       "  ('bread', 0.754692792892456),\n",
       "  ('sugar', 0.7499966621398926),\n",
       "  ('milk', 0.747291088104248)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recipe 2\n",
    "create_recipe(model,words2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['salt', 'rice', 'garlic', 'curry'],\n",
       " [('pepper', 0.8080397844314575),\n",
       "  ('onion', 0.7354762554168701),\n",
       "  ('sugar', 0.7323951125144958),\n",
       "  ('chili', 0.7305773496627808),\n",
       "  ('sauce', 0.7283685803413391)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recipe 3\n",
    "create_recipe(model,words3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['banana', 'curry', 'rice'],\n",
       " [('bean', 0.7592380046844482),\n",
       "  ('coconut', 0.7236413359642029),\n",
       "  ('potato', 0.7230938673019409),\n",
       "  ('corn', 0.720425009727478),\n",
       "  ('sugar', 0.7177786231040955)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recipe 4\n",
    "create_recipe(model,words4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilizing Word2Vec\n",
    "modelW2V = Word2Vec(ingredients, min_count=2,size= 300, window =5, sg = 1,negative=15,hs=0,iter=1)\n",
    "wordVector = modelW2V.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['salt', 'rice', 'garlic', 'curry'],\n",
       " [('wasabi', 0.9117565751075745),\n",
       "  ('heaping', 0.9116976261138916),\n",
       "  ('pinches', 0.9083110094070435),\n",
       "  ('garam', 0.9080813527107239)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recipe based on Word2Vec\n",
    "create_recipe(wordVector,words3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('plum', 0.8487071394920349),\n",
       " ('Italianstyle', 0.8190993070602417),\n",
       " ('vineripened', 0.8152807950973511),\n",
       " ('jalapeños', 0.8149125576019287),\n",
       " ('anchovies', 0.813493013381958),\n",
       " ('heaping', 0.8043901324272156),\n",
       " ('stewed', 0.8025950193405151),\n",
       " ('puréed', 0.7992008924484253),\n",
       " ('purée', 0.7988237738609314),\n",
       " ('brine', 0.7939332127571106)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MoSt similar Word2Vec tomato\n",
    "modelW2V.most_similar(\"tomato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function TSNE Scatterplot\n",
    "def display_closestwords_tsnescatterplot(model, words, samples):\n",
    "    \n",
    "    arr = np.empty((0,300), dtype='f')\n",
    "    word_labels = []\n",
    "\n",
    "    # get close words\n",
    "    for word in words:\n",
    "        \n",
    "        word_labels.append(word)\n",
    "        close_words = model.similar_by_word(word)[:samples]\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "        arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "        for wrd_score in close_words:\n",
    "            wrd_vector = model[wrd_score[0]]\n",
    "            word_labels.append(wrd_score[0])\n",
    "            arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=10, perplexity=5)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['salt', 'rice', 'garlic', 'curry']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scatterplot for words3 and 3 most similar words each\n",
    "display_closestwords_tsnescatterplot(wordVector, words3,3)\n",
    "words3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
